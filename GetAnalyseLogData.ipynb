{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Fetch data from ElasticSearch bulk API using the Index, Timestamp range ,fetch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Elastic search url\n",
    "elasticsearch = \"https://miesqa.clouddqt.capitalone.com/\"\n",
    "\n",
    "# Index \n",
    "index=\"card_asvpartnerservicing_partnernotification_app-td-000001\"\n",
    "# Operation to perform\n",
    "operation=\"/_search\"\n",
    "url=f\"{elasticsearch}{index}{operation}\"\n",
    "\n",
    "# Number of records to search in one go\n",
    "querystring = {\"size\":\"10\"}\n",
    "\n",
    "# Start time for the search\n",
    "start_ts=\"2019-08-25T23:58:00.000Z\"\n",
    "# End time for the search\n",
    "end_ts=\"2019-08-25T23:58:30.000Z\"\n",
    "\n",
    "data=[]\n",
    "document_id=[]\n",
    "logstash_time=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Create a POST Payload using the above filter criterea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload= {\"sort\" : [{ \"logstash_time\" : {\"order\" : \"asc\"}},{\"_id\": \"asc\"} ],\"query\": {\"bool\": {\"filter\": [{\"range\": {\"@timestamp\": {\"gte\": start_ts,\"lte\": end_ts}}}]}}}                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Create the Header for the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Content-Type': \"application/json\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Hit the API to get the fetch count records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get the response\n",
    "response = requests.request(\"POST\", url, data=json.dumps(payload) , headers=headers, params=querystring,verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Validate if the retrun record size is lesser than the Fetch size count set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the total response if less than the fetch size \n",
    "\n",
    "size=int(querystring[\"size\"])\n",
    "total_hits=response.json()[\"hits\"][\"total\"]\n",
    "total_hits\n",
    "if size > total_hits:\n",
    "    size = total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Get the doucment id and sort value for the last record for pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values for the pagination for next set of request to pull\n",
    "search_val1=response.json()[\"hits\"][\"hits\"][size-1][\"sort\"][0]\n",
    "search_val2=response.json()[\"hits\"][\"hits\"][size-1][\"sort\"][1]\n",
    "total_hits=response.json()[\"hits\"][\"total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Get the next records till the end of the total size fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "i=size\n",
    "while i <= total_hits:        \n",
    "        j=0\n",
    "        search_val1=response.json()[\"hits\"][\"hits\"][size-1][\"sort\"][0]\n",
    "        search_val2=response.json()[\"hits\"][\"hits\"][size-1][\"sort\"][1]\n",
    "        while j < size and response.json()[\"hits\"][\"hits\"][j][\"_source\"][\"message\"]:\n",
    "            document_id.append(response.json()[\"hits\"][\"hits\"][j][\"_id\"])\n",
    "            data.append(response.json()[\"hits\"][\"hits\"][j][\"_source\"][\"message\"])\n",
    "            j=j+1\n",
    "        payload= {\"search_after\": [search_val1,search_val2],\"sort\" : [{ \"logstash_time\" : {\"order\" : \"asc\"}},{\"_id\": \"asc\"} ],\"query\": {\"bool\": {\"filter\": [{\"range\": {\"@timestamp\": {\"gte\": start_ts,\"lte\": end_ts}}}]}}}                \n",
    "        response = requests.request(\"POST\", url, data=json.dumps(payload), headers=headers, params=querystring,verify=False)\n",
    "        size=len(response.json()[\"hits\"][\"hits\"])\n",
    "        if size == 0:\n",
    "            break;\n",
    "        i=i + size;\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Put the data into a dataframe and convert into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kibana_output = pd.DataFrame(data,document_id) \n",
    "kibana_output.reset_index()\n",
    "kibana_output.to_csv(\"./outputData/logData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Analyse the above created logs file and create a training data for sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./outputData/Training.csv', 'w') as csvfile:\n",
    "    filewriter = csv.writer(csvfile)\n",
    "    filewriter.writerow(['SequenceNumber','IsPan','bin','sixteendigits','fifteendigits','fourteendigits','startwithfive','startwithfour'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Analyse the log data for  \n",
    "# a) If Bin if present \n",
    "# b) If Bin Yes - If it has 14,15 or 16 digits \n",
    "# c) If digits present check if it starts with 5,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty doc id\n"
     ]
    }
   ],
   "source": [
    "# Import the list of bins\n",
    "from binConfig import searchstrList\n",
    "\n",
    "bin_found=False\n",
    "# sequence = 0\n",
    "with open('./outputData/logData.csv','r') as file:\n",
    "    for line in file:\n",
    "#         print(line.split(\",\")[0])  \n",
    "        for s in searchstrList:\n",
    "            \n",
    "            if s in line:\n",
    "                bin_found=True\n",
    "                index=line.find(s)\n",
    "                if index >=0:\n",
    "  \n",
    "                    dataitem19=(line[index:index+19])\n",
    "                    dataitem16=(line[index:index+16])\n",
    "                    dataitem15=(line[index:index+15])\n",
    "                    dataitem14=(line[index:index+14])\n",
    "                    \n",
    "                    dataList=[]\n",
    "                    dataList.insert(0,line.split(\",\")[0])\n",
    "\n",
    "                    dataList.insert(2,s)\n",
    "                    if dataitem16.isnumeric()== True:\n",
    "                        dataList.insert(3,1)    \n",
    "                    else:\n",
    "                        dataList.insert(3,0)\n",
    "                        \n",
    "                    if dataitem15.isnumeric()== True:\n",
    "                        dataList.insert(4,1)    \n",
    "                    else:\n",
    "                        dataList.insert(4,0)\n",
    "                        \n",
    "                    if dataitem14.isnumeric()== True:\n",
    "                        dataList.insert(5,1)\n",
    "                        dataList.insert(1,1)        \n",
    "                    else:\n",
    "                        dataList.insert(5,0)\n",
    "                        dataList.insert(1,0)\n",
    "                    if s[1] == '5':\n",
    "                        dataList.insert(6,1)    \n",
    "                    else:\n",
    "                        dataList.insert(6,0)\n",
    "                   \n",
    "                    if s[1] == '4':\n",
    "                        dataList.insert(7,1)\n",
    "                    else:\n",
    "                        dataList.insert(7,0)\n",
    "                    with open('./outputData/Training.csv', 'a') as csvfile:\n",
    "                        filewriter = csv.writer(csvfile)\n",
    "                        filewriter.writerow(dataList)\n",
    "                        \n",
    "        if bin_found == False:\n",
    "            dataList=[]\n",
    "            dataList.insert(0,line.split(\",\")[0])\n",
    "            if line.split(\",\")[0] == '':\n",
    "                print(\"empty doc id\")\n",
    "            else:\n",
    "                dataList.insert(1,0)    \n",
    "#                 dataList.insert(2,0)    \n",
    "                dataList.insert(3,0)    \n",
    "                dataList.insert(4,0)    \n",
    "                dataList.insert(5,0)    \n",
    "                dataList.insert(6,0)    \n",
    "                dataList.insert(7,0)   \n",
    "                with open('./outputData/Training.csv', 'a') as csvfile:\n",
    "                    filewriter = csv.writer(csvfile)\n",
    "                    filewriter.writerow(dataList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
